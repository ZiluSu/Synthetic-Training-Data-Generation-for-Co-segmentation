{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2\n",
    "## Advanced Foreground Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import skimage.io as io\n",
    "import skimage.transform as transform\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Foreground via Mask R-CNN and GrabCut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    return np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_image_transform():\n",
    "    transforms = [T.ToTensor()]\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grabcut_mask(image, grabcut_mask):\n",
    "    bgd_model = np.zeros((1, 65), np.float64)\n",
    "    fgd_model = np.zeros((1, 65), np.float64)\n",
    "    grabcut_mask, _, _ = cv2.grabCut(\n",
    "        image,\n",
    "        grabcut_mask,\n",
    "        None,\n",
    "        bgd_model,\n",
    "        fgd_model,\n",
    "        5,\n",
    "        cv2.GC_INIT_WITH_MASK\n",
    "    )\n",
    "    return np.where((grabcut_mask == 2) | (grabcut_mask == 0), 0, 1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForeGroundExtractor:\n",
    "    # Mask R-CNN model\n",
    "    def __init__(self, mrcnn_pre_process, mrcnn_confidence=0.8, grabcut_foreground_confidence=0.8, detect_object_label=1):\n",
    "        self.mrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "        self.mrcnn.eval()\n",
    "        self.mrcnn_confidence = mrcnn_confidence\n",
    "        self.grabcut_foreground = grabcut_foreground_confidence\n",
    "        self.trans = mrcnn_pre_process\n",
    "        self.detect_object_label = detect_object_label\n",
    "    \n",
    "    # Object dection with bounding box as the onput of GrabCut\n",
    "    def mrcnn_output2grabcut_input(self, output):\n",
    "        boxes = output[0]['boxes'].detach().numpy()\n",
    "        masks = output[0]['masks'].detach().numpy()\n",
    "        labels = output[0]['labels'].detach().numpy()\n",
    "        scores = output[0]['scores'].detach().numpy()\n",
    "        boxes = boxes[(self.mrcnn_confidence < scores) & \n",
    "                      (labels == self.detect_object_label)].astype(np.uint64)\n",
    "        masks = masks[(self.mrcnn_confidence < scores) & \n",
    "                      (labels == self.detect_object_label)]\n",
    "\n",
    "        grab_mask = np.zeros(masks.shape[2:], np.uint8)\n",
    "        for b in boxes:\n",
    "            grab_mask[b[1]:b[3]:, b[0]:b[2]] = cv2.GC_PR_BGD\n",
    "        for m in masks:\n",
    "            grab_mask[self.grabcut_foreground < m[0]] = cv2.GC_FGD\n",
    "        return grab_mask\n",
    "\n",
    "    def detect_foreground(self, image):\n",
    "        output = self.mrcnn([self.trans(Image.fromarray(image))])\n",
    "        grabcut_input = self.mrcnn_output2grabcut_input(output)\n",
    "        if not (grabcut_input == cv2.GC_FGD).any():\n",
    "            return np.zeros(image.shape[:2]).astype(np.uint8)\n",
    "        return create_grabcut_mask(image, grabcut_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, the same as Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foregroundAug(foreground):\n",
    "    angle = np.random.randint(-10,10)*(np.pi/180.0) # Convert to radians\n",
    "    zoom = np.random.random()*0.4 + 0.8 # Zoom in range [0.8,1.2)\n",
    "    t_x = np.random.randint(0, int(foreground.shape[1]/3))\n",
    "    t_y = np.random.randint(0, int(foreground.shape[0]/3))\n",
    "\n",
    "    tform = transform.AffineTransform(scale=(zoom,zoom),\n",
    "                                rotation=angle,\n",
    "                                translation=(t_x, t_y))\n",
    "    foreground = transform.warp(foreground, tform.inverse)\n",
    "\n",
    "    # Random horizontal flip with 0.5 probability\n",
    "    if(np.random.randint(0,100)>=50):\n",
    "        foreground = foreground[:, ::-1]\n",
    "        \n",
    "    return foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getForegroundMask(foreground):\n",
    "    mask_new = foreground.copy()[:,:,0]\n",
    "    mask_new[mask_new>0] = 1\n",
    "    return mask_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(foreground, mask, background):\n",
    "    background = transform.resize(background, foreground.shape[:2])\n",
    "\n",
    "    background = background*(1 - mask.reshape(foreground.shape[0], foreground.shape[1], 1))\n",
    "\n",
    "    composed_image = background + foreground    \n",
    "  \n",
    "    return composed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO label\n",
    "# Reference: \n",
    "# https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "\n",
    "fge = ForeGroundExtractor(get_simple_image_transform(), detect_object_label=17) # 17 is cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data2/input/image/cat1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torchvision/ops/boxes.py:101: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1595629449223/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  keep = keep.nonzero().squeeze(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data2/input/image/cat0.jpg\n",
      "data2/input/image/cat2.jpg\n",
      "data2/input/image/cat3.jpg\n"
     ]
    }
   ],
   "source": [
    "# Save the extracted foreground first\n",
    "DIR = \"data2/input/image/\"\n",
    "imagePaths = [os.path.join(DIR, f) for f in os.listdir(DIR)]\n",
    "if DIR + '.DS_Store' in imagePaths:\n",
    "    imagePaths.remove(DIR + '.DS_Store')\n",
    "\n",
    "for i in range(len(imagePaths)):  \n",
    "    print(imagePaths[i])\n",
    "    src = read_image(imagePaths[i])\n",
    "    Image.fromarray(src)\n",
    "    \n",
    "    foreground_mask = fge.detect_foreground(src)\n",
    "    Image.fromarray(255 * foreground_mask)\n",
    "    \n",
    "    result = np.zeros(src.shape).astype(np.uint8)\n",
    "    result[foreground_mask == 1] = src[foreground_mask == 1]\n",
    "    \n",
    "    plt.imsave(\"data2/input/foreground/{}.png\".format(i), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data2/input/foreground/2.png\n",
      "data2/input/foreground/3.png\n",
      "data2/input/foreground/1.png\n",
      "data2/input/foreground/0.png\n"
     ]
    }
   ],
   "source": [
    "fore_DIR = \"data2/input/foreground/\"\n",
    "forePaths = [os.path.join(fore_DIR, f) for f in os.listdir(fore_DIR)]\n",
    "if fore_DIR + '.DS_Store' in forePaths:\n",
    "    forePaths.remove(fore_DIR + '.DS_Store')\n",
    "\n",
    "for i in range(len(forePaths)): \n",
    "    print(forePaths[i])\n",
    "    foreground = Image.open(forePaths[i])\n",
    "    foreground = foreground.convert('RGB')\n",
    "    foreground = np.array(foreground)\n",
    "\n",
    "    back_DIR = \"data2/input/background/cloister/\"\n",
    "    backPaths = [os.path.join(back_DIR, f) for f in os.listdir(back_DIR)]\n",
    "    if back_DIR + '.DS_Store' in backPaths:\n",
    "        backPaths.remove(back_DIR + '.DS_Store')\n",
    "    \n",
    "    for j in range(len(backPaths)):  \n",
    "        \n",
    "        background = io.imread(backPaths[j],plugin='matplotlib')/255.0  \n",
    "        background = np.array(background)\n",
    "        \n",
    "        if background.shape[2] != foreground.shape[2]:\n",
    "            continue\n",
    "            \n",
    "        foreground_new = foregroundAug(foreground)\n",
    "        mask_new = getForegroundMask(foreground_new)\n",
    "        plt.imsave(\"data2/output/mask/{}_{}.png\".format(i, j), mask_new)\n",
    "        \n",
    "        composed_image = compose(foreground_new, mask_new, background)\n",
    "        \n",
    "        composed_image = tf.clip_by_value(composed_image, 0.0, 1.0)\n",
    "        \n",
    "        plt.imsave(\"data2/output/image/{}_{}.png\".format(i, j), \n",
    "                   composed_image.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Reference: \n",
    "https://github.com/stonzw/human_foreground_extractor     \n",
    "https://github.com/virafpatrawala/Synthetic-Image-Datasets \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
